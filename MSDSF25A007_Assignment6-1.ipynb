{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqOieS1QZ70A+8oeIK7G+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memrranmian/my-first-repo1/blob/main/MSDSF25A007_Assignment6-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "XDj49h3Y9-HE",
        "outputId": "ca76e15f-5772-422d-e082-603c297cad31"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-694532451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Q1. Multi-Source Data Consolidation + Inconsistency Resolution\n",
        "# You are given three datasets of hospital patient records:\n",
        "# â€¢ patients_master.csv: patient_id, name, dob, gender\n",
        "# â€¢ visit_records.csv: visit_id, patient_id, visit_date, diagnosis, doctor\n",
        "# â€¢ billing.csv: visit_id, amount, discount, paid_status\n",
        "# Tasks:\n",
        "# â€¢ Merge the three datasets into a single patient-level dataframe such that:\n",
        "# o All visits and billings must be preserved (even if no matching patient exists).\n",
        "# o For missing patient info, create a placeholder row with \"Unknown\" fields.\n",
        "# â€¢ Identify conflicting patient information (e.g., same patient_id with different gender or\n",
        "# dob).\n",
        "# Write Pandas code to:\n",
        "# o Detect conflicts\n",
        "# o Resolve them by keeping the most frequent value per patient\n",
        "# o Log all conflicts to a separate DataFrame for auditing\n",
        "# â€¢ Find the top 10 patients by total billed amount, accounting for:\n",
        "# o Missing discount (treat as 0)\n",
        "# o Missing amount (set to column median)\n",
        "\n",
        "\n",
        "\n",
        "# First, we need to install the required libraries\n",
        "# We'll use these commands in our terminal (command prompt):\n",
        "# pip install pandas numpy faker\n",
        "\n",
        "# Now let's start writing our code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "\n",
        "# ============================================\n",
        "# PART 1: CREATE FAKE DATA\n",
        "# ============================================\n",
        "\n",
        "# Let me explain what we're doing here:\n",
        "# 1. We're importing pandas (for data handling), numpy (for numbers), and Faker (to create fake data)\n",
        "# 2. Faker helps us create realistic but fake data for practice\n",
        "\n",
        "# Create a Faker object to generate fake data\n",
        "fake = Faker()\n",
        "\n",
        "# Set a random seed so we get the same results each time\n",
        "np.random.seed(42)\n",
        "\n",
        "# Let's create some fake patients first\n",
        "patients_data = []\n",
        "for i in range(1, 101):  # Creating 100 patients\n",
        "    patients_data.append({\n",
        "        'patient_id': f'P{i:03d}',  # Creates IDs like P001, P002, etc.\n",
        "        'name': fake.name(),\n",
        "        'dob': fake.date_of_birth(minimum_age=18, maximum_age=90).strftime('%Y-%m-%d'),\n",
        "        'gender': np.random.choice(['M', 'F', 'Other'], p=[0.48, 0.48, 0.04])\n",
        "    })\n",
        "\n",
        "# Create the patients_master dataframe\n",
        "patients_master = pd.DataFrame(patients_data)\n",
        "\n",
        "# Now let's create visit records\n",
        "visit_data = []\n",
        "visit_ids = []\n",
        "for i in range(1, 201):  # Creating 200 visits\n",
        "    visit_id = f'V{i:03d}'\n",
        "    visit_ids.append(visit_id)\n",
        "\n",
        "    # Randomly pick a patient (some might not exist in patients_master)\n",
        "    patient_id = f'P{np.random.randint(1, 120):03d}'  # Goes up to 120, but we only have 100 patients\n",
        "\n",
        "    visit_data.append({\n",
        "        'visit_id': visit_id,\n",
        "        'patient_id': patient_id,\n",
        "        'visit_date': fake.date_between(start_date='-2y', end_date='today').strftime('%Y-%m-%d'),\n",
        "        'diagnosis': np.random.choice(['Flu', 'Cold', 'Injury', 'Checkup', 'Surgery', 'Other']),\n",
        "        'doctor': fake.name()\n",
        "    })\n",
        "\n",
        "# Create the visit_records dataframe\n",
        "visit_records = pd.DataFrame(visit_data)\n",
        "\n",
        "# Now let's create billing records\n",
        "billing_data = []\n",
        "for visit_id in visit_ids:\n",
        "    # Create some randomness in billing\n",
        "    amount = np.random.choice([100, 200, 300, 400, 500, 1000, np.nan], p=[0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
        "    discount = np.random.choice([0, 5, 10, 20, np.nan], p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
        "    paid_status = np.random.choice(['Paid', 'Unpaid', 'Partial'], p=[0.7, 0.2, 0.1])\n",
        "\n",
        "    billing_data.append({\n",
        "        'visit_id': visit_id,\n",
        "        'amount': amount,\n",
        "        'discount': discount,\n",
        "        'paid_status': paid_status\n",
        "    })\n",
        "\n",
        "# Create the billing dataframe\n",
        "billing = pd.DataFrame(billing_data)\n",
        "\n",
        "# Let's add some conflicts in patient data (for demonstration)\n",
        "# We'll create duplicate patient records with different information\n",
        "conflict_data = [\n",
        "    {'patient_id': 'P001', 'name': 'John Conflict', 'dob': '1990-01-01', 'gender': 'M'},\n",
        "    {'patient_id': 'P001', 'name': 'John Doe', 'dob': '1990-01-01', 'gender': 'F'},  # Gender conflict\n",
        "    {'patient_id': 'P002', 'name': 'Jane Smith', 'dob': '1985-05-15', 'gender': 'F'},\n",
        "    {'patient_id': 'P002', 'name': 'Jane Smith', 'dob': '1980-05-15', 'gender': 'F'},  # DOB conflict\n",
        "]\n",
        "\n",
        "# Add these to patients_master to create conflicts\n",
        "patients_master = pd.concat([patients_master, pd.DataFrame(conflict_data)], ignore_index=True)\n",
        "\n",
        "# ============================================\n",
        "# PART 2: MERGE THE DATASETS\n",
        "# ============================================\n",
        "\n",
        "print(\"Step 1: Merging all the datasets together\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# First, let me show you what each dataset looks like\n",
        "print(\"\\nPatients Master (first 5 rows):\")\n",
        "print(patients_master.head())\n",
        "print(\"\\nVisit Records (first 5 rows):\")\n",
        "print(visit_records.head())\n",
        "print(\"\\nBilling Records (first 5 rows):\")\n",
        "print(billing.head())\n",
        "\n",
        "# Now, let's merge visit records with billing records\n",
        "# We use 'visit_id' as the common column\n",
        "print(\"\\nMerging visit records with billing records...\")\n",
        "visits_with_billing = pd.merge(visit_records, billing, on='visit_id', how='left')\n",
        "print(\"Visits with billing (first 5 rows):\")\n",
        "print(visits_with_billing.head())\n",
        "\n",
        "# Now, let's merge this with patient information\n",
        "# We use 'patient_id' as the common column\n",
        "print(\"\\nMerging with patient information...\")\n",
        "# We use 'how=left' to keep all visits even if patient doesn't exist\n",
        "all_data = pd.merge(visits_with_billing, patients_master, on='patient_id', how='left')\n",
        "print(\"All merged data (first 5 rows):\")\n",
        "print(all_data.head())\n",
        "\n",
        "# For patients who don't exist in patients_master, fill with \"Unknown\"\n",
        "print(\"\\nFilling missing patient info with 'Unknown'...\")\n",
        "all_data['name'] = all_data['name'].fillna('Unknown')\n",
        "all_data['dob'] = all_data['dob'].fillna('Unknown')\n",
        "all_data['gender'] = all_data['gender'].fillna('Unknown')\n",
        "\n",
        "print(\"\\nFinal merged dataset (first 10 rows):\")\n",
        "print(all_data.head(10))\n",
        "\n",
        "# ============================================\n",
        "# PART 3: FIND AND FIX CONFLICTS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\nStep 2: Finding and fixing conflicting information\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# First, let's find all unique patient IDs\n",
        "unique_patients = patients_master['patient_id'].unique()\n",
        "print(f\"\\nWe have {len(unique_patients)} unique patient IDs in our master data\")\n",
        "\n",
        "# Let's check for conflicts - patients with different information\n",
        "conflicts = []\n",
        "\n",
        "# We'll check each patient one by one\n",
        "for patient_id in unique_patients:\n",
        "    # Get all records for this patient\n",
        "    patient_records = patients_master[patients_master['patient_id'] == patient_id]\n",
        "\n",
        "    # If patient has more than one record\n",
        "    if len(patient_records) > 1:\n",
        "        # Check if there are differences in gender\n",
        "        unique_genders = patient_records['gender'].unique()\n",
        "        if len(unique_genders) > 1:\n",
        "            conflicts.append({\n",
        "                'patient_id': patient_id,\n",
        "                'field': 'gender',\n",
        "                'values': list(unique_genders),\n",
        "                'conflict_type': 'Multiple values'\n",
        "            })\n",
        "\n",
        "        # Check if there are differences in date of birth\n",
        "        unique_dobs = patient_records['dob'].unique()\n",
        "        if len(unique_dobs) > 1:\n",
        "            conflicts.append({\n",
        "                'patient_id': patient_id,\n",
        "                'field': 'dob',\n",
        "                'values': list(unique_dobs),\n",
        "                'conflict_type': 'Multiple values'\n",
        "            })\n",
        "\n",
        "        # Check if there are differences in name\n",
        "        unique_names = patient_records['name'].unique()\n",
        "        if len(unique_names) > 1:\n",
        "            conflicts.append({\n",
        "                'patient_id': patient_id,\n",
        "                'field': 'name',\n",
        "                'values': list(unique_names),\n",
        "                'conflict_type': 'Multiple values'\n",
        "            })\n",
        "\n",
        "# Create a conflicts dataframe for logging\n",
        "if conflicts:\n",
        "    conflicts_df = pd.DataFrame(conflicts)\n",
        "    print(\"\\nFound these conflicts:\")\n",
        "    print(conflicts_df)\n",
        "else:\n",
        "    conflicts_df = pd.DataFrame()  # Empty dataframe if no conflicts\n",
        "    print(\"\\nNo conflicts found!\")\n",
        "\n",
        "# Now, let's resolve conflicts by keeping the most frequent value\n",
        "print(\"\\nResolving conflicts by keeping the most common value...\")\n",
        "\n",
        "# Create a clean version of patients_master\n",
        "clean_patients = patients_master.copy()\n",
        "\n",
        "# We'll fix each conflict one by one\n",
        "if not conflicts_df.empty:\n",
        "    for _, conflict in conflicts_df.iterrows():\n",
        "        patient_id = conflict['patient_id']\n",
        "        field = conflict['field']\n",
        "\n",
        "        # Get all values for this field for this patient\n",
        "        field_values = patients_master[patients_master['patient_id'] == patient_id][field]\n",
        "\n",
        "        # Find the most common value\n",
        "        most_common_value = field_values.mode()[0]  # mode() finds most frequent value\n",
        "\n",
        "        # Keep only one record per patient with the most common value\n",
        "        # First, get all records for this patient\n",
        "        patient_mask = clean_patients['patient_id'] == patient_id\n",
        "\n",
        "        # Keep the first record and update with most common value\n",
        "        first_index = clean_patients[patient_mask].index[0]\n",
        "        clean_patients.loc[first_index, field] = most_common_value\n",
        "\n",
        "        # Remove duplicate records\n",
        "        # We'll keep the first record and drop the rest\n",
        "        duplicates_to_drop = clean_patients[patient_mask].index[1:]\n",
        "        clean_patients = clean_patients.drop(duplicates_to_drop)\n",
        "\n",
        "print(\"\\nCleaned patient data (showing only previously conflicting patients):\")\n",
        "conflicting_ids = conflicts_df['patient_id'].unique() if not conflicts_df.empty else []\n",
        "if len(conflicting_ids) > 0:\n",
        "    print(clean_patients[clean_patients['patient_id'].isin(conflicting_ids)])\n",
        "\n",
        "# ============================================\n",
        "# PART 4: FIND TOP 10 PATIENTS BY BILLED AMOUNT\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\nStep 3: Finding top 10 patients by total billed amount\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# First, let's handle missing values in the billing data\n",
        "print(\"\\nHandling missing values in billing data...\")\n",
        "\n",
        "# Make a copy of our merged data to work with\n",
        "analysis_data = all_data.copy()\n",
        "\n",
        "# Handle missing discount - treat as 0\n",
        "print(\"1. Missing discounts will be treated as 0\")\n",
        "analysis_data['discount'] = analysis_data['discount'].fillna(0)\n",
        "\n",
        "# Handle missing amount - set to median\n",
        "print(\"2. Missing amounts will be set to the median amount\")\n",
        "median_amount = analysis_data['amount'].median()\n",
        "analysis_data['amount'] = analysis_data['amount'].fillna(median_amount)\n",
        "\n",
        "print(f\"Median amount used for missing values: ${median_amount:.2f}\")\n",
        "\n",
        "# Calculate net amount after discount\n",
        "print(\"\\n3. Calculating net amount (amount - discount)...\")\n",
        "analysis_data['net_amount'] = analysis_data['amount'] - analysis_data['discount']\n",
        "\n",
        "# Group by patient to get total amount per patient\n",
        "print(\"\\n4. Grouping by patient to get totals...\")\n",
        "patient_totals = analysis_data.groupby('patient_id').agg({\n",
        "    'name': 'first',  # Get the patient name\n",
        "    'net_amount': 'sum'  # Sum up all net amounts\n",
        "}).reset_index()\n",
        "\n",
        "# Sort by total amount in descending order\n",
        "patient_totals = patient_totals.sort_values('net_amount', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 patients by total billed amount:\")\n",
        "top_10_patients = patient_totals.head(10)\n",
        "print(top_10_patients)\n",
        "\n",
        "# ============================================\n",
        "# PART 5: SAVE RESULTS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\nStep 4: Saving the results\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Save the merged data\n",
        "all_data.to_csv('merged_patient_data.csv', index=False)\n",
        "print(\"âœ“ Saved merged data to 'merged_patient_data.csv'\")\n",
        "\n",
        "# Save the conflicts log\n",
        "if not conflicts_df.empty:\n",
        "    conflicts_df.to_csv('conflicts_log.csv', index=False)\n",
        "    print(\"âœ“ Saved conflicts log to 'conflicts_log.csv'\")\n",
        "else:\n",
        "    print(\"âœ“ No conflicts to save\")\n",
        "\n",
        "# Save the cleaned patient data\n",
        "clean_patients.to_csv('cleaned_patients_master.csv', index=False)\n",
        "print(\"âœ“ Saved cleaned patient data to 'cleaned_patients_master.csv'\")\n",
        "\n",
        "# Save top 10 patients\n",
        "top_10_patients.to_csv('top_10_patients_by_billing.csv', index=False)\n",
        "print(\"âœ“ Saved top 10 patients to 'top_10_patients_by_billing.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PROCESS COMPLETE!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Time-Series Irregularity Detection + Resampling\n",
        "# A sensor dataset contains data from 40 IoT devices:\n",
        "# Columns:\n",
        "# device_id, timestamp, temperature, humidity\n",
        "# The sampling rate varies between devices because of connectivity issues.\n",
        "# Tasks:\n",
        "# â€¢ For each device, resample data to 1-minute intervals using:\n",
        "# o Forward-fill for temperature\n",
        "# o Cubic interpolation for humidity\n",
        "# â€¢ Detect abnormal devices where:\n",
        "# o More than 25% of data points were interpolated, OR\n",
        "# o Any continuous gap > 20 minutes occurred\n",
        "# â€¢ For each abnormal device, compute the time windows of instability and export them to\n",
        "# a dataframe with:\n",
        "# o device_id, gap_start, gap_end, gap_duration_minutes\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Time-Series Irregularity Detection + Resampling\n",
        "A sensor dataset contains data from 40 IoT devices with varying sampling rates.\n",
        "\"\"\"\n",
        "\n",
        "# Import Required Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "\n",
        "def generate_sensor_data(num_devices=40, base_start_date='2024-01-01', duration_hours=24):\n",
        "    \"\"\"\n",
        "    Generate synthetic sensor data with irregular sampling rates for IoT devices.\n",
        "\n",
        "    Parameters:\n",
        "    - num_devices: Number of IoT devices\n",
        "    - base_start_date: Starting date for data generation\n",
        "    - duration_hours: Duration of data collection in hours\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with columns: device_id, timestamp, temperature, humidity\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    start_date = pd.to_datetime(base_start_date)\n",
        "\n",
        "    for device_id in range(1, num_devices + 1):\n",
        "        current_time = start_date\n",
        "        end_time = start_date + timedelta(hours=duration_hours)\n",
        "\n",
        "        # Randomly decide if this device will have connectivity issues\n",
        "        has_issues = random.random() < 0.3  # 30% of devices have issues\n",
        "\n",
        "        while current_time < end_time:\n",
        "            # Generate temperature and humidity readings\n",
        "            temperature = round(random.uniform(15.0, 35.0), 2)\n",
        "            humidity = round(random.uniform(30.0, 90.0), 2)\n",
        "\n",
        "            data.append({\n",
        "                'device_id': f'device_{device_id:03d}',\n",
        "                'timestamp': current_time,\n",
        "                'temperature': temperature,\n",
        "                'humidity': humidity\n",
        "            })\n",
        "\n",
        "            # Variable sampling rate (simulate connectivity issues)\n",
        "            if has_issues and random.random() < 0.15:  # Occasional large gaps\n",
        "                gap_minutes = random.choice([25, 30, 35, 40])  # Large gaps\n",
        "                current_time += timedelta(minutes=gap_minutes)\n",
        "            elif has_issues and random.random() < 0.3:  # Frequent small gaps\n",
        "                gap_minutes = random.uniform(2, 10)\n",
        "                current_time += timedelta(minutes=gap_minutes)\n",
        "            else:  # Normal sampling\n",
        "                gap_minutes = random.uniform(0.5, 3)\n",
        "                current_time += timedelta(minutes=gap_minutes)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df.sort_values(['device_id', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def resample_device_data(device_df, device_id):\n",
        "    \"\"\"\n",
        "    Resample a single device's data to 1-minute intervals.\n",
        "\n",
        "    Parameters:\n",
        "    - device_df: DataFrame for a single device\n",
        "    - device_id: ID of the device\n",
        "\n",
        "    Returns:\n",
        "    - Resampled DataFrame with interpolation metadata\n",
        "    \"\"\"\n",
        "    # Set timestamp as index\n",
        "    device_df = device_df.set_index('timestamp').sort_index()\n",
        "\n",
        "    # Create 1-minute interval range\n",
        "    start_time = device_df.index.min()\n",
        "    end_time = device_df.index.max()\n",
        "    full_range = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
        "\n",
        "    # Reindex to 1-minute intervals\n",
        "    resampled = device_df.reindex(full_range)\n",
        "\n",
        "    # Track which values were interpolated\n",
        "    original_mask = resampled['temperature'].notna()\n",
        "\n",
        "    # Forward-fill for temperature\n",
        "    resampled['temperature'] = resampled['temperature'].ffill()\n",
        "\n",
        "    # Cubic interpolation for humidity (with fallback to linear if insufficient points)\n",
        "    try:\n",
        "        # Try cubic interpolation first\n",
        "        resampled['humidity'] = resampled['humidity'].interpolate(method='cubic')\n",
        "    except (ValueError, TypeError):\n",
        "        # Fall back to polynomial or linear interpolation if cubic fails\n",
        "        try:\n",
        "            resampled['humidity'] = resampled['humidity'].interpolate(method='polynomial', order=2)\n",
        "        except (ValueError, TypeError):\n",
        "            # Final fallback to linear interpolation\n",
        "            resampled['humidity'] = resampled['humidity'].interpolate(method='linear')\n",
        "\n",
        "    # Add device_id back\n",
        "    resampled['device_id'] = device_id\n",
        "\n",
        "    # Calculate interpolation statistics\n",
        "    total_points = len(resampled)\n",
        "    interpolated_points = (~original_mask).sum()\n",
        "    interpolation_percentage = (interpolated_points / total_points) * 100\n",
        "\n",
        "    return resampled.reset_index().rename(columns={'index': 'timestamp'}), interpolation_percentage, original_mask\n",
        "\n",
        "\n",
        "def detect_gaps(original_mask, timestamps):\n",
        "    \"\"\"\n",
        "    Detect continuous gaps in the original data.\n",
        "\n",
        "    Parameters:\n",
        "    - original_mask: Boolean mask indicating original (non-interpolated) data points\n",
        "    - timestamps: Array of timestamps\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples: (gap_start, gap_end, gap_duration_minutes)\n",
        "    \"\"\"\n",
        "    gaps = []\n",
        "    in_gap = False\n",
        "    gap_start = None\n",
        "\n",
        "    for i, (is_original, timestamp) in enumerate(zip(original_mask, timestamps)):\n",
        "        if not is_original and not in_gap:\n",
        "            # Start of a gap\n",
        "            in_gap = True\n",
        "            gap_start = timestamp\n",
        "        elif is_original and in_gap:\n",
        "            # End of a gap\n",
        "            gap_end = timestamps[i - 1]\n",
        "            gap_duration = pd.Timedelta(gap_end - gap_start).total_seconds() / 60\n",
        "            gaps.append((gap_start, gap_end, gap_duration))\n",
        "            in_gap = False\n",
        "\n",
        "    # Handle case where data ends in a gap\n",
        "    if in_gap:\n",
        "        gap_end = timestamps[-1]\n",
        "        gap_duration = pd.Timedelta(gap_end - gap_start).total_seconds() / 60\n",
        "        gaps.append((gap_start, gap_end, gap_duration))\n",
        "\n",
        "    return gaps\n",
        "\n",
        "\n",
        "def process_all_devices(df):\n",
        "    \"\"\"\n",
        "    Process all devices: resample and detect abnormalities.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Original sensor DataFrame\n",
        "\n",
        "    Returns:\n",
        "    - resampled_data: Dictionary of resampled DataFrames by device\n",
        "    - abnormal_devices: List of abnormal device IDs\n",
        "    - instability_windows: DataFrame with instability time windows\n",
        "    \"\"\"\n",
        "    resampled_data = {}\n",
        "    abnormal_devices = []\n",
        "    instability_records = []\n",
        "\n",
        "    device_ids = df['device_id'].unique()\n",
        "\n",
        "    print(f\"Processing {len(device_ids)} devices...\\n\")\n",
        "\n",
        "    for device_id in device_ids:\n",
        "        device_df = df[df['device_id'] == device_id].copy()\n",
        "\n",
        "        # Resample the device data\n",
        "        resampled_df, interp_pct, original_mask = resample_device_data(device_df, device_id)\n",
        "        resampled_data[device_id] = resampled_df\n",
        "\n",
        "        # Detect gaps\n",
        "        gaps = detect_gaps(original_mask.values, resampled_df['timestamp'].values)\n",
        "\n",
        "        # Check for abnormality conditions\n",
        "        has_high_interpolation = interp_pct > 25\n",
        "        has_large_gap = any(gap_duration > 20 for _, _, gap_duration in gaps)\n",
        "\n",
        "        if has_high_interpolation or has_large_gap:\n",
        "            abnormal_devices.append(device_id)\n",
        "\n",
        "            print(f\"[!] {device_id} - ABNORMAL\")\n",
        "            print(f\"   Interpolation: {interp_pct:.2f}%\")\n",
        "            print(f\"   Number of gaps: {len(gaps)}\")\n",
        "\n",
        "            # Record all gaps for abnormal devices (instability windows)\n",
        "            for gap_start, gap_end, gap_duration in gaps:\n",
        "                instability_records.append({\n",
        "                    'device_id': device_id,\n",
        "                    'gap_start': gap_start,\n",
        "                    'gap_end': gap_end,\n",
        "                    'gap_duration_minutes': round(gap_duration, 2)\n",
        "                })\n",
        "                print(f\"   Gap: {gap_start} to {gap_end} ({gap_duration:.2f} min)\")\n",
        "            print()\n",
        "        else:\n",
        "            print(f\"[OK] {device_id} - Normal (Interpolation: {interp_pct:.2f}%)\")\n",
        "\n",
        "    # Create instability windows DataFrame\n",
        "    instability_df = pd.DataFrame(instability_records)\n",
        "\n",
        "    return resampled_data, abnormal_devices, instability_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Time-Series Irregularity Detection + Resampling\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    # Step 1: Generate sensor data\n",
        "    print(\"Step 1: Generating sensor data for 40 IoT devices...\")\n",
        "    sensor_data = generate_sensor_data(num_devices=40, duration_hours=24)\n",
        "    print(f\"Generated {len(sensor_data)} data points\")\n",
        "    print(f\"Date range: {sensor_data['timestamp'].min()} to {sensor_data['timestamp'].max()}\")\n",
        "    print()\n",
        "\n",
        "    # Display sample of original data\n",
        "    print(\"Sample of original data:\")\n",
        "    print(sensor_data.head(10))\n",
        "    print()\n",
        "\n",
        "    # Step 2: Process all devices\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Step 2: Resampling and detecting abnormalities...\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    resampled_data, abnormal_devices, instability_df = process_all_devices(sensor_data)\n",
        "\n",
        "    # Step 3: Summary\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Total devices: {len(sensor_data['device_id'].unique())}\")\n",
        "    print(f\"Abnormal devices: {len(abnormal_devices)}\")\n",
        "    print(f\"Abnormal device IDs: {', '.join(abnormal_devices)}\")\n",
        "    print()\n",
        "\n",
        "    # Step 4: Display instability windows\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Instability Windows (Abnormal Devices)\")\n",
        "    print(\"=\" * 70)\n",
        "    if not instability_df.empty:\n",
        "        print(instability_df.to_string(index=False))\n",
        "        print()\n",
        "        print(f\"Total instability windows: {len(instability_df)}\")\n",
        "    else:\n",
        "        print(\"No abnormal devices detected.\")\n",
        "    print()\n",
        "\n",
        "    # Step 5: Export results\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Exporting Results\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Export instability windows\n",
        "    instability_df.to_csv('instability_windows.csv', index=False)\n",
        "    print(\"[OK] Instability windows exported to: instability_windows.csv\")\n",
        "\n",
        "    # Export sample resampled data (first abnormal device if exists)\n",
        "    if abnormal_devices:\n",
        "        sample_device = abnormal_devices[0]\n",
        "        resampled_data[sample_device].to_csv(f'resampled_{sample_device}.csv', index=False)\n",
        "        print(f\"[OK] Sample resampled data exported to: resampled_{sample_device}.csv\")\n",
        "\n",
        "    # Export all resampled data\n",
        "    all_resampled = pd.concat(resampled_data.values(), ignore_index=True)\n",
        "    all_resampled.to_csv('all_resampled_data.csv', index=False)\n",
        "    print(\"[OK] All resampled data exported to: all_resampled_data.csv\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Processing Complete!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return sensor_data, resampled_data, abnormal_devices, instability_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sensor_data, resampled_data, abnormal_devices, instability_df = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "qbp6SmuiBGTX",
        "outputId": "6db714a9-f81f-4a21-f98f-d87e2a614c3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4204646460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. High-Level Grouping + Multi-Stage Aggregation\n",
        "# You have an e-commerce dataset:\n",
        "# Columns:\n",
        "# order_id, customer_id, product_id, price, discount, category, order_timestamp\n",
        "# Tasks:\n",
        "# â€¢ Compute category-wise â€œEffective Price Variance,â€ defined as:\n",
        "# ï¿½\n",
        "# ï¿½ð‘ƒð‘‰ =ð‘‰ð‘Žð‘Ÿ(ð‘ð‘Ÿð‘–ð‘ð‘’ âˆ’ð‘‘ð‘–ð‘ ð‘ð‘œð‘¢ð‘›ð‘¡)\n",
        "# â€¢ For each customer, compute:\n",
        "# o Average inter-purchase time (in hours)\n",
        "# o Category with highest spending\n",
        "# o Rolling 3-purchase moving average of spending (sorted by timestamp)\n",
        "# â€¢ Identify customers showing anomalous behaviour, defined as:\n",
        "# o Rolling average increasing by > 300% compared to previous window\n",
        "# o At least 5 purchases in last 30 days\n",
        "# Create a final dataframe with these customers and their metrics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Detailed Analysis of Anomalous Customers\n",
        "This script provides deeper insights into the anomalous customer behavior.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_anomalous_customers():\n",
        "    \"\"\"\n",
        "    Load and analyze anomalous customers in detail.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    ecommerce_df = pd.read_csv('ecommerce_data.csv')\n",
        "    ecommerce_df['order_timestamp'] = pd.to_datetime(ecommerce_df['order_timestamp'])\n",
        "    ecommerce_df['effective_price'] = ecommerce_df['price'] - ecommerce_df['discount']\n",
        "\n",
        "    anomalous_df = pd.read_csv('anomalous_customers.csv')\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Detailed Analysis of Anomalous Customers\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    for idx, row in anomalous_df.iterrows():\n",
        "        customer_id = row['customer_id']\n",
        "\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"Customer: {customer_id}\")\n",
        "        print(f\"{'=' * 80}\")\n",
        "\n",
        "        # Get customer's orders\n",
        "        customer_orders = ecommerce_df[ecommerce_df['customer_id'] == customer_id].sort_values('order_timestamp')\n",
        "\n",
        "        print(f\"\\nTotal Purchases: {len(customer_orders)}\")\n",
        "        print(f\"Total Spending: ${customer_orders['effective_price'].sum():.2f}\")\n",
        "        print(f\"Average Order Value: ${customer_orders['effective_price'].mean():.2f}\")\n",
        "        print(f\"Date Range: {customer_orders['order_timestamp'].min()} to {customer_orders['order_timestamp'].max()}\")\n",
        "\n",
        "        # Calculate rolling average\n",
        "        customer_orders['rolling_avg_3'] = customer_orders['effective_price'].rolling(window=3, min_periods=1).mean()\n",
        "        customer_orders['prev_rolling_avg'] = customer_orders['rolling_avg_3'].shift(1)\n",
        "        customer_orders['pct_change'] = ((customer_orders['rolling_avg_3'] - customer_orders['prev_rolling_avg']) /\n",
        "                                          customer_orders['prev_rolling_avg'] * 100)\n",
        "\n",
        "        print(f\"\\n--- Purchase Timeline (Last 10 Purchases) ---\")\n",
        "        recent_orders = customer_orders.tail(10)[['order_timestamp', 'category', 'effective_price', 'rolling_avg_3', 'pct_change']]\n",
        "        recent_orders['order_timestamp'] = recent_orders['order_timestamp'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "        print(recent_orders.to_string(index=False))\n",
        "\n",
        "        # Find the spike\n",
        "        max_spike_idx = customer_orders['pct_change'].idxmax()\n",
        "        if pd.notna(max_spike_idx):\n",
        "            spike_row = customer_orders.loc[max_spike_idx]\n",
        "            print(f\"\\n--- Maximum Spike Details ---\")\n",
        "            print(f\"Date: {spike_row['order_timestamp']}\")\n",
        "            print(f\"Category: {spike_row['category']}\")\n",
        "            print(f\"Order Value: ${spike_row['effective_price']:.2f}\")\n",
        "            print(f\"Rolling Avg (3-purchase): ${spike_row['rolling_avg_3']:.2f}\")\n",
        "            print(f\"Previous Rolling Avg: ${spike_row['prev_rolling_avg']:.2f}\")\n",
        "            print(f\"Percentage Increase: {spike_row['pct_change']:.2f}%\")\n",
        "\n",
        "        # Category breakdown\n",
        "        print(f\"\\n--- Spending by Category ---\")\n",
        "        category_spending = customer_orders.groupby('category')['effective_price'].agg(['sum', 'count', 'mean'])\n",
        "        category_spending.columns = ['Total Spending', 'Num Orders', 'Avg Order Value']\n",
        "        category_spending = category_spending.sort_values('Total Spending', ascending=False)\n",
        "        print(category_spending.to_string())\n",
        "\n",
        "        # Recent activity (last 30 days)\n",
        "        last_date = ecommerce_df['order_timestamp'].max()\n",
        "        recent_30 = customer_orders[customer_orders['order_timestamp'] >= (last_date - pd.Timedelta(days=30))]\n",
        "        print(f\"\\n--- Last 30 Days Activity ---\")\n",
        "        print(f\"Number of Purchases: {len(recent_30)}\")\n",
        "        print(f\"Total Spending: ${recent_30['effective_price'].sum():.2f}\")\n",
        "        print(f\"Average Order Value: ${recent_30['effective_price'].mean():.2f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Comparison: Anomalous vs Normal Customers\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Load all customer metrics\n",
        "    all_metrics = pd.read_csv('customer_metrics.csv')\n",
        "    normal_customers = all_metrics[all_metrics['is_anomalous'] == False]\n",
        "\n",
        "    print(f\"\\nNormal Customers (n={len(normal_customers)}):\")\n",
        "    print(f\"  Avg Total Spending: ${normal_customers['total_spending'].mean():.2f}\")\n",
        "    print(f\"  Avg Purchases: {normal_customers['total_purchases'].mean():.2f}\")\n",
        "    print(f\"  Avg Inter-Purchase Time: {normal_customers['avg_inter_purchase_time_hours'].mean():.2f} hours\")\n",
        "    print(f\"  Avg Max Spike: {normal_customers['max_rolling_avg_spike_pct'].mean():.2f}%\")\n",
        "\n",
        "    print(f\"\\nAnomalous Customers (n={len(anomalous_df)}):\")\n",
        "    print(f\"  Avg Total Spending: ${anomalous_df['total_spending'].mean():.2f}\")\n",
        "    print(f\"  Avg Purchases: {anomalous_df['total_purchases'].mean():.2f}\")\n",
        "    print(f\"  Avg Inter-Purchase Time: {anomalous_df['avg_inter_purchase_time_hours'].mean():.2f} hours\")\n",
        "    print(f\"  Avg Max Spike: {anomalous_df['max_rolling_avg_spike_pct'].mean():.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Analysis Complete!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_anomalous_customers()\n"
      ],
      "metadata": {
        "id": "3-GI2m62GTJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Multi-Key Merge + Window Analytics\n",
        "# A transportation dataset contains:\n",
        "# 1. trips.csv\n",
        "# trip_id, driver_id, start_time, end_time, distance_km, rating\n",
        "# 2. fuel_logs.csv\n",
        "# driver_id, log_time, fuel_liters, fuel_price\n",
        "# 3. incidents.csv\n",
        "# driver_id, incident_time, incident_type\n",
        "# Tasks:\n",
        "# â€¢ Combine all data to produce a per-driver performance dataframe with:\n",
        "# o Total trips\n",
        "# o Average rating (exclude NaN)\n",
        "# o Total incidents\n",
        "# o Fuel consumption aligned to the nearest trip end_time using a custom merge-as\n",
        "# of tolerance of 30 minutes\n",
        "# â€¢ Using window functions, compute:\n",
        "# o Rolling 7-trip average rating per driver\n",
        "# o Rolling sum of incidents per 10 trips\n",
        "# â€¢ Identify drivers with declining performance, defined as:\n",
        "# o Last 7-trip rating average < 3\n",
        "# o Incident rate in last 30 days > 2\n",
        "# o Fuel consumption increasing by > 50% compared to previous month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Q4. Multi-Key Merge + Window Analytics\n",
        "Transportation dataset analysis with merge-asof and window functions.\n",
        "\"\"\"\n",
        "\n",
        "# Import Required Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "\n",
        "def generate_trips_data(num_drivers=50, num_trips=2000):\n",
        "    \"\"\"\n",
        "    Generate trips.csv data.\n",
        "    Columns: trip_id, driver_id, start_time, end_time, distance_km, rating\n",
        "    \"\"\"\n",
        "    trips = []\n",
        "    start_date = datetime(2024, 1, 1)\n",
        "\n",
        "    for trip_id in range(1, num_trips + 1):\n",
        "        driver_id = f'D{random.randint(1, num_drivers):03d}'\n",
        "\n",
        "        # Random start time within the year\n",
        "        days_offset = random.uniform(0, 365)\n",
        "        start_time = start_date + timedelta(days=days_offset,\n",
        "                                            hours=random.randint(0, 23),\n",
        "                                            minutes=random.randint(0, 59))\n",
        "\n",
        "        # Trip duration: 10 minutes to 3 hours\n",
        "        duration_minutes = random.uniform(10, 180)\n",
        "        end_time = start_time + timedelta(minutes=duration_minutes)\n",
        "\n",
        "        # Distance: 1 to 100 km\n",
        "        distance_km = round(random.uniform(1, 100), 2)\n",
        "\n",
        "        # Rating: 1-5, with some NaN values (10% missing)\n",
        "        if random.random() < 0.1:\n",
        "            rating = np.nan\n",
        "        else:\n",
        "            # Some drivers are better rated\n",
        "            driver_num = int(driver_id[1:])\n",
        "            if driver_num <= 10:  # First 10 drivers have declining performance\n",
        "                # Recent trips have lower ratings\n",
        "                base_rating = 4.5 - (days_offset / 365) * 2  # Decline over time\n",
        "                rating = max(1, min(5, base_rating + random.uniform(-0.5, 0.5)))\n",
        "            else:\n",
        "                rating = random.uniform(3.5, 5.0)\n",
        "            rating = round(rating, 1)\n",
        "\n",
        "        trips.append({\n",
        "            'trip_id': f'T{trip_id:06d}',\n",
        "            'driver_id': driver_id,\n",
        "            'start_time': start_time,\n",
        "            'end_time': end_time,\n",
        "            'distance_km': distance_km,\n",
        "            'rating': rating\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(trips)\n",
        "    return df.sort_values(['driver_id', 'end_time']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def generate_fuel_logs(trips_df, num_drivers=50):\n",
        "    \"\"\"\n",
        "    Generate fuel_logs.csv data.\n",
        "    Columns: driver_id, log_time, fuel_liters, fuel_price\n",
        "    \"\"\"\n",
        "    fuel_logs = []\n",
        "\n",
        "    # Generate fuel logs for each driver\n",
        "    for driver_id in trips_df['driver_id'].unique():\n",
        "        driver_trips = trips_df[trips_df['driver_id'] == driver_id]\n",
        "\n",
        "        # Generate fuel logs near trip end times (within 30 minutes)\n",
        "        for _, trip in driver_trips.sample(frac=0.7).iterrows():  # 70% of trips have fuel logs\n",
        "            # Log time is near end_time (within 30 minutes)\n",
        "            offset_minutes = random.uniform(-30, 30)\n",
        "            log_time = trip['end_time'] + timedelta(minutes=offset_minutes)\n",
        "\n",
        "            # Fuel consumption based on distance\n",
        "            base_fuel = trip['distance_km'] * 0.08  # ~8L per 100km\n",
        "\n",
        "            # Some drivers have increasing fuel consumption over time\n",
        "            driver_num = int(driver_id[1:])\n",
        "            if driver_num <= 10:  # Declining performance drivers\n",
        "                # Fuel consumption increases over time\n",
        "                days_since_start = (trip['end_time'] - datetime(2024, 1, 1)).days\n",
        "                fuel_multiplier = 1 + (days_since_start / 365) * 0.6  # Up to 60% increase\n",
        "                fuel_liters = base_fuel * fuel_multiplier\n",
        "            else:\n",
        "                fuel_liters = base_fuel * random.uniform(0.9, 1.1)\n",
        "\n",
        "            fuel_liters = round(fuel_liters, 2)\n",
        "            fuel_price = round(random.uniform(1.5, 2.5), 2)  # Price per liter\n",
        "\n",
        "            fuel_logs.append({\n",
        "                'driver_id': driver_id,\n",
        "                'log_time': log_time,\n",
        "                'fuel_liters': fuel_liters,\n",
        "                'fuel_price': fuel_price\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(fuel_logs)\n",
        "    return df.sort_values(['driver_id', 'log_time']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def generate_incidents(trips_df, num_drivers=50):\n",
        "    \"\"\"\n",
        "    Generate incidents.csv data.\n",
        "    Columns: driver_id, incident_time, incident_type\n",
        "    \"\"\"\n",
        "    incidents = []\n",
        "    incident_types = ['Speeding', 'Harsh Braking', 'Accident', 'Traffic Violation', 'Customer Complaint']\n",
        "\n",
        "    for driver_id in trips_df['driver_id'].unique():\n",
        "        driver_trips = trips_df[trips_df['driver_id'] == driver_id]\n",
        "\n",
        "        # Declining performance drivers have more incidents\n",
        "        driver_num = int(driver_id[1:])\n",
        "        if driver_num <= 10:\n",
        "            # More incidents, especially in recent months\n",
        "            num_incidents = random.randint(5, 15)\n",
        "            # Concentrate incidents in last 60 days\n",
        "            recent_date = datetime(2024, 12, 31)\n",
        "            for _ in range(num_incidents):\n",
        "                if random.random() < 0.6:  # 60% in last 60 days\n",
        "                    days_back = random.uniform(0, 60)\n",
        "                else:\n",
        "                    days_back = random.uniform(0, 365)\n",
        "\n",
        "                incident_time = recent_date - timedelta(days=days_back,\n",
        "                                                        hours=random.randint(0, 23),\n",
        "                                                        minutes=random.randint(0, 59))\n",
        "\n",
        "                incidents.append({\n",
        "                    'driver_id': driver_id,\n",
        "                    'incident_time': incident_time,\n",
        "                    'incident_type': random.choice(incident_types)\n",
        "                })\n",
        "        else:\n",
        "            # Normal drivers have fewer incidents\n",
        "            num_incidents = random.randint(0, 5)\n",
        "            for _ in range(num_incidents):\n",
        "                # Random time within the year\n",
        "                random_trip = driver_trips.sample(1).iloc[0]\n",
        "                offset_hours = random.uniform(-24, 24)\n",
        "                incident_time = random_trip['end_time'] + timedelta(hours=offset_hours)\n",
        "\n",
        "                incidents.append({\n",
        "                    'driver_id': driver_id,\n",
        "                    'incident_time': incident_time,\n",
        "                    'incident_type': random.choice(incident_types)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(incidents)\n",
        "    return df.sort_values(['driver_id', 'incident_time']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def merge_asof_fuel_to_trips(trips_df, fuel_df, tolerance_minutes=30):\n",
        "    \"\"\"\n",
        "    Merge fuel logs to trips using merge_asof with 30-minute tolerance.\n",
        "    Process each driver separately to avoid sorting issues.\n",
        "    \"\"\"\n",
        "    # Prepare dataframes\n",
        "    trips = trips_df.copy()\n",
        "    fuel = fuel_df.copy()\n",
        "\n",
        "    # Ensure datetime types\n",
        "    trips['end_time'] = pd.to_datetime(trips['end_time'])\n",
        "    fuel['log_time'] = pd.to_datetime(fuel['log_time'])\n",
        "\n",
        "    # Merge each driver separately\n",
        "    tolerance = pd.Timedelta(minutes=tolerance_minutes)\n",
        "    merged_list = []\n",
        "\n",
        "    for driver_id in trips['driver_id'].unique():\n",
        "        driver_trips = trips[trips['driver_id'] == driver_id].sort_values('end_time').reset_index(drop=True)\n",
        "        driver_fuel = fuel[fuel['driver_id'] == driver_id].sort_values('log_time').reset_index(drop=True)\n",
        "\n",
        "        if len(driver_fuel) > 0:\n",
        "            # Merge asof for this driver\n",
        "            driver_merged = pd.merge_asof(driver_trips, driver_fuel,\n",
        "                                         left_on='end_time',\n",
        "                                         right_on='log_time',\n",
        "                                         tolerance=tolerance,\n",
        "                                         direction='nearest',\n",
        "                                         suffixes=('', '_fuel'))\n",
        "        else:\n",
        "            # No fuel logs for this driver\n",
        "            driver_merged = driver_trips.copy()\n",
        "            driver_merged['log_time'] = pd.NaT\n",
        "            driver_merged['fuel_liters'] = np.nan\n",
        "            driver_merged['fuel_price'] = np.nan\n",
        "\n",
        "        # Ensure driver_id is present\n",
        "        if 'driver_id' not in driver_merged.columns:\n",
        "            driver_merged['driver_id'] = driver_id\n",
        "\n",
        "        merged_list.append(driver_merged)\n",
        "\n",
        "    # Combine all drivers\n",
        "    merged = pd.concat(merged_list, ignore_index=True)\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "\n",
        "def compute_driver_performance(trips_df, fuel_df, incidents_df):\n",
        "    \"\"\"\n",
        "    Compute per-driver performance metrics.\n",
        "    \"\"\"\n",
        "    # Merge fuel data to trips\n",
        "    trips_with_fuel = merge_asof_fuel_to_trips(trips_df, fuel_df)\n",
        "\n",
        "    # Compute basic metrics per driver\n",
        "    driver_metrics = []\n",
        "\n",
        "    for driver_id in trips_df['driver_id'].unique():\n",
        "        driver_trips = trips_with_fuel[trips_with_fuel['driver_id'] == driver_id]\n",
        "        driver_incidents = incidents_df[incidents_df['driver_id'] == driver_id]\n",
        "\n",
        "        # Total trips\n",
        "        total_trips = len(driver_trips)\n",
        "\n",
        "        # Average rating (exclude NaN)\n",
        "        avg_rating = driver_trips['rating'].mean()\n",
        "\n",
        "        # Total incidents\n",
        "        total_incidents = len(driver_incidents)\n",
        "\n",
        "        # Total fuel consumption (sum of aligned fuel logs)\n",
        "        total_fuel = driver_trips['fuel_liters'].sum()\n",
        "\n",
        "        driver_metrics.append({\n",
        "            'driver_id': driver_id,\n",
        "            'total_trips': total_trips,\n",
        "            'avg_rating': round(avg_rating, 2) if not pd.isna(avg_rating) else np.nan,\n",
        "            'total_incidents': total_incidents,\n",
        "            'total_fuel_liters': round(total_fuel, 2) if not pd.isna(total_fuel) else 0\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(driver_metrics)\n",
        "\n",
        "\n",
        "def compute_window_analytics(trips_df, incidents_df):\n",
        "    \"\"\"\n",
        "    Compute window functions:\n",
        "    - Rolling 7-trip average rating per driver\n",
        "    - Rolling sum of incidents per 10 trips\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for driver_id in trips_df['driver_id'].unique():\n",
        "        driver_trips = trips_df[trips_df['driver_id'] == driver_id].sort_values('end_time').copy()\n",
        "        driver_incidents = incidents_df[incidents_df['driver_id'] == driver_id]\n",
        "\n",
        "        # Rolling 7-trip average rating\n",
        "        driver_trips['rolling_7_avg_rating'] = driver_trips['rating'].rolling(window=7, min_periods=1).mean()\n",
        "\n",
        "        # For each trip, count incidents in the last 10 trips\n",
        "        driver_trips['trip_number'] = range(1, len(driver_trips) + 1)\n",
        "\n",
        "        # Count incidents per trip window\n",
        "        incident_counts = []\n",
        "        for idx, trip in driver_trips.iterrows():\n",
        "            # Get last 10 trips (including current)\n",
        "            trip_num = trip['trip_number']\n",
        "            start_trip = max(1, trip_num - 9)\n",
        "\n",
        "            # Get trips in this window\n",
        "            window_trips = driver_trips[\n",
        "                (driver_trips['trip_number'] >= start_trip) &\n",
        "                (driver_trips['trip_number'] <= trip_num)\n",
        "            ]\n",
        "\n",
        "            # Count incidents during this window\n",
        "            window_start = window_trips['start_time'].min()\n",
        "            window_end = window_trips['end_time'].max()\n",
        "\n",
        "            incidents_in_window = len(driver_incidents[\n",
        "                (driver_incidents['incident_time'] >= window_start) &\n",
        "                (driver_incidents['incident_time'] <= window_end)\n",
        "            ])\n",
        "\n",
        "            incident_counts.append(incidents_in_window)\n",
        "\n",
        "        driver_trips['rolling_10_incidents'] = incident_counts\n",
        "\n",
        "        # Get last trip metrics\n",
        "        last_trip = driver_trips.iloc[-1]\n",
        "\n",
        "        results.append({\n",
        "            'driver_id': driver_id,\n",
        "            'last_7_trip_avg_rating': round(last_trip['rolling_7_avg_rating'], 2) if not pd.isna(last_trip['rolling_7_avg_rating']) else np.nan,\n",
        "            'last_10_trip_incidents': last_trip['rolling_10_incidents']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def identify_declining_drivers(trips_df, fuel_df, incidents_df):\n",
        "    \"\"\"\n",
        "    Identify drivers with declining performance:\n",
        "    - Last 7-trip rating average < 3\n",
        "    - Incident rate in last 30 days > 2\n",
        "    - Fuel consumption increasing by > 50% compared to previous month\n",
        "    \"\"\"\n",
        "    declining_drivers = []\n",
        "    current_date = trips_df['end_time'].max()\n",
        "\n",
        "    for driver_id in trips_df['driver_id'].unique():\n",
        "        driver_trips = trips_df[trips_df['driver_id'] == driver_id].sort_values('end_time')\n",
        "        driver_fuel = fuel_df[fuel_df['driver_id'] == driver_id].sort_values('log_time')\n",
        "        driver_incidents = incidents_df[incidents_df['driver_id'] == driver_id]\n",
        "\n",
        "        # Skip drivers with too few trips\n",
        "        if len(driver_trips) < 7:\n",
        "            continue\n",
        "\n",
        "        # 1. Last 7-trip rating average < 3\n",
        "        last_7_trips = driver_trips.tail(7)\n",
        "        last_7_avg_rating = last_7_trips['rating'].mean()\n",
        "        has_low_rating = last_7_avg_rating < 3\n",
        "\n",
        "        # 2. Incident rate in last 30 days > 2\n",
        "        last_30_days = current_date - timedelta(days=30)\n",
        "        recent_incidents = driver_incidents[driver_incidents['incident_time'] >= last_30_days]\n",
        "        incident_rate_30d = len(recent_incidents)\n",
        "        has_high_incidents = incident_rate_30d > 2\n",
        "\n",
        "        # 3. Fuel consumption increasing by > 50% compared to previous month\n",
        "        last_month_start = current_date - timedelta(days=30)\n",
        "        last_month_end = current_date\n",
        "        prev_month_start = current_date - timedelta(days=60)\n",
        "        prev_month_end = current_date - timedelta(days=30)\n",
        "\n",
        "        last_month_fuel = driver_fuel[\n",
        "            (driver_fuel['log_time'] >= last_month_start) &\n",
        "            (driver_fuel['log_time'] <= last_month_end)\n",
        "        ]['fuel_liters'].sum()\n",
        "\n",
        "        prev_month_fuel = driver_fuel[\n",
        "            (driver_fuel['log_time'] >= prev_month_start) &\n",
        "            (driver_fuel['log_time'] <= prev_month_end)\n",
        "        ]['fuel_liters'].sum()\n",
        "\n",
        "        if prev_month_fuel > 0:\n",
        "            fuel_increase_pct = ((last_month_fuel - prev_month_fuel) / prev_month_fuel) * 100\n",
        "        else:\n",
        "            fuel_increase_pct = 0\n",
        "\n",
        "        has_fuel_increase = fuel_increase_pct > 50\n",
        "\n",
        "        # Check if declining\n",
        "        is_declining = has_low_rating and has_high_incidents and has_fuel_increase\n",
        "\n",
        "        if is_declining:\n",
        "            declining_drivers.append({\n",
        "                'driver_id': driver_id,\n",
        "                'last_7_trip_avg_rating': round(last_7_avg_rating, 2),\n",
        "                'incidents_last_30_days': incident_rate_30d,\n",
        "                'prev_month_fuel_liters': round(prev_month_fuel, 2),\n",
        "                'last_month_fuel_liters': round(last_month_fuel, 2),\n",
        "                'fuel_increase_pct': round(fuel_increase_pct, 2),\n",
        "                'is_declining': True\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(declining_drivers)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Transportation Analysis: Multi-Key Merge + Window Analytics\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # Step 1: Generate datasets\n",
        "    print(\"Step 1: Generating transportation datasets...\")\n",
        "\n",
        "    trips_df = generate_trips_data(num_drivers=50, num_trips=2000)\n",
        "    print(f\"Generated trips.csv: {len(trips_df)} trips\")\n",
        "\n",
        "    fuel_df = generate_fuel_logs(trips_df, num_drivers=50)\n",
        "    print(f\"Generated fuel_logs.csv: {len(fuel_df)} fuel logs\")\n",
        "\n",
        "    incidents_df = generate_incidents(trips_df, num_drivers=50)\n",
        "    print(f\"Generated incidents.csv: {len(incidents_df)} incidents\")\n",
        "    print()\n",
        "\n",
        "    # Save to CSV\n",
        "    trips_df.to_csv('trips.csv', index=False)\n",
        "    fuel_df.to_csv('fuel_logs.csv', index=False)\n",
        "    incidents_df.to_csv('incidents.csv', index=False)\n",
        "    print(\"[OK] All datasets saved to CSV files\")\n",
        "    print()\n",
        "\n",
        "    # Display samples\n",
        "    print(\"Sample from trips.csv:\")\n",
        "    print(trips_df.head(5))\n",
        "    print()\n",
        "\n",
        "    print(\"Sample from fuel_logs.csv:\")\n",
        "    print(fuel_df.head(5))\n",
        "    print()\n",
        "\n",
        "    print(\"Sample from incidents.csv:\")\n",
        "    print(incidents_df.head(5))\n",
        "    print()\n",
        "\n",
        "    # Step 2: Compute driver performance\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Step 2: Computing Per-Driver Performance Metrics\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    driver_performance = compute_driver_performance(trips_df, fuel_df, incidents_df)\n",
        "    print(driver_performance.head(10))\n",
        "    print()\n",
        "\n",
        "    # Step 3: Window analytics\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Step 3: Computing Window Analytics\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    window_metrics = compute_window_analytics(trips_df, incidents_df)\n",
        "    print(window_metrics.head(10))\n",
        "    print()\n",
        "\n",
        "    # Step 4: Identify declining drivers\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Step 4: Identifying Drivers with Declining Performance\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    declining_drivers = identify_declining_drivers(trips_df, fuel_df, incidents_df)\n",
        "\n",
        "    if len(declining_drivers) > 0:\n",
        "        print(f\"Found {len(declining_drivers)} drivers with declining performance:\")\n",
        "        print()\n",
        "        print(declining_drivers.to_string(index=False))\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No drivers with declining performance detected.\")\n",
        "        print()\n",
        "\n",
        "    # Step 5: Export results\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Exporting Results\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    driver_performance.to_csv('driver_performance.csv', index=False)\n",
        "    print(\"[OK] Driver performance exported to: driver_performance.csv\")\n",
        "\n",
        "    window_metrics.to_csv('window_metrics.csv', index=False)\n",
        "    print(\"[OK] Window metrics exported to: window_metrics.csv\")\n",
        "\n",
        "    declining_drivers.to_csv('declining_drivers.csv', index=False)\n",
        "    print(\"[OK] Declining drivers exported to: declining_drivers.csv\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Analysis Complete!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return trips_df, fuel_df, incidents_df, driver_performance, window_metrics, declining_drivers\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trips_df, fuel_df, incidents_df, driver_performance, window_metrics, declining_drivers = main()\n"
      ],
      "metadata": {
        "id": "JUYah0kbuSXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Large-Scale Optimization + Complex Missing-Value Strategies\n",
        "# A genomics lab produces a 5M-row dataset:\n",
        "# Columns:\n",
        "# gene_id, sample_id, expression_level, qc_flag, batch, lab_technician\n",
        "# Tasks:\n",
        "# â€¢ Remove batch-specific outliers using median absolute deviation (MAD) per batch.\n",
        "# â€¢ Expression levels are missing (NaN) for ~18% rows.\n",
        "# o Impute missing values using multi-step strategy:\n",
        "# â–ª Step 1: For rows where qc_flag == \"BAD\", impute with batch median\n",
        "# â–ª Step 2: For others, impute using a KNN-style Pandas approach based on:\n",
        "# â€¢ Other genes in the same batch\n",
        "# â€¢ Technicians who processed similar samples\n",
        "# â€¢ The dataset must be optimized for memory:\n",
        "# o Convert categorical columns to category dtype\n",
        "# o Downcast floats and ints\n",
        "# o Chunk-process 500k rows at a time while merging KNN imputations back\n",
        "# â€¢ After cleaning, compute the gene stability index:\n",
        "# ï¿½\n",
        "# ï¿½ð‘ºð‘° = ð‘ºð’•ð’…(ð’†ð’™ð’‘ð’“ð’†ð’”ð’”ð’Šð’ð’_ð’ð’†ð’—ð’†ð’)\n",
        "#  ð‘´ð’†ð’‚ð’(ð’†ð’™ð’‘ð’“ð’†ð’”ð’”ð’Šð’ð’_ð’ð’†ð’—ð’†ð’)\n",
        "#  and identify the top 100 unstable genes\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Q5. Large-Scale Optimization + Complex Missing-Value Strategies\n",
        "Genomics lab dataset with 5M rows - memory-optimized processing with advanced imputation.\n",
        "\"\"\"\n",
        "\n",
        "# Import Required Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "\n",
        "def generate_genomics_data(num_rows=5_000_000, num_genes=10000, num_samples=500):\n",
        "    \"\"\"\n",
        "    Generate large-scale genomics dataset with memory optimization.\n",
        "\n",
        "    Parameters:\n",
        "    - num_rows: Total number of rows (default: 5M)\n",
        "    - num_genes: Number of unique genes\n",
        "    - num_samples: Number of unique samples\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with optimized dtypes\n",
        "    \"\"\"\n",
        "    print(\"Generating genomics dataset...\")\n",
        "    print(f\"Target size: {num_rows:,} rows\")\n",
        "\n",
        "    # Generate data in chunks to avoid memory issues\n",
        "    chunk_size = 500_000\n",
        "    num_chunks = num_rows // chunk_size\n",
        "\n",
        "    batches = ['Batch_A', 'Batch_B', 'Batch_C', 'Batch_D', 'Batch_E']\n",
        "    qc_flags = ['GOOD', 'BAD', 'MODERATE']\n",
        "    technicians = [f'Tech_{i:02d}' for i in range(1, 21)]  # 20 technicians\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        print(f\"  Generating chunk {chunk_idx + 1}/{num_chunks}...\")\n",
        "\n",
        "        # Use numpy for memory efficiency - generate in smaller batches\n",
        "        batch_size = 50_000\n",
        "        mini_chunks = []\n",
        "\n",
        "        for mini_idx in range(chunk_size // batch_size):\n",
        "            gene_ids = np.random.randint(1, num_genes + 1, size=batch_size)\n",
        "            sample_ids = np.random.randint(1, num_samples + 1, size=batch_size)\n",
        "\n",
        "            mini_data = {\n",
        "                'gene_id': pd.Categorical([f'GENE_{gid:05d}' for gid in gene_ids]),\n",
        "                'sample_id': pd.Categorical([f'SAMPLE_{sid:04d}' for sid in sample_ids]),\n",
        "                'expression_level': np.random.lognormal(mean=5, sigma=2, size=batch_size),\n",
        "                'qc_flag': pd.Categorical(np.random.choice(qc_flags, size=batch_size, p=[0.70, 0.15, 0.15])),\n",
        "                'batch': pd.Categorical(np.random.choice(batches, size=batch_size)),\n",
        "                'lab_technician': pd.Categorical(np.random.choice(technicians, size=batch_size))\n",
        "            }\n",
        "\n",
        "            mini_df = pd.DataFrame(mini_data)\n",
        "\n",
        "            # Introduce ~18% missing values in expression_level\n",
        "            missing_mask = np.random.random(batch_size) < 0.18\n",
        "            mini_df.loc[missing_mask, 'expression_level'] = np.nan\n",
        "\n",
        "            # Add some outliers (will be removed later)\n",
        "            outlier_mask = np.random.random(batch_size) < 0.02\n",
        "            mini_df.loc[outlier_mask, 'expression_level'] = np.random.uniform(1000, 10000, outlier_mask.sum())\n",
        "\n",
        "            mini_chunks.append(mini_df)\n",
        "\n",
        "            del mini_data, gene_ids, sample_ids\n",
        "            gc.collect()\n",
        "\n",
        "        # Combine mini chunks into chunk\n",
        "        chunk_df = pd.concat(mini_chunks, ignore_index=True)\n",
        "        chunks.append(chunk_df)\n",
        "\n",
        "        # Clear memory\n",
        "        del mini_chunks\n",
        "        gc.collect()\n",
        "\n",
        "    # Combine all chunks\n",
        "    print(\"  Combining chunks...\")\n",
        "    df = pd.concat(chunks, ignore_index=True)\n",
        "    del chunks\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Generated {len(df):,} rows\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_memory(df):\n",
        "    \"\"\"\n",
        "    Optimize DataFrame memory usage.\n",
        "    - Convert categorical columns to category dtype\n",
        "    - Downcast floats and ints\n",
        "    \"\"\"\n",
        "    print(\"\\nOptimizing memory usage...\")\n",
        "\n",
        "    # Get initial memory usage\n",
        "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"  Initial memory: {initial_memory:.2f} MB\")\n",
        "\n",
        "    # Convert categorical columns\n",
        "    categorical_cols = ['gene_id', 'sample_id', 'qc_flag', 'batch', 'lab_technician']\n",
        "    for col in categorical_cols:\n",
        "        df[col] = df[col].astype('category')\n",
        "\n",
        "    # Downcast numeric columns\n",
        "    df['expression_level'] = pd.to_numeric(df['expression_level'], downcast='float')\n",
        "\n",
        "    # Get final memory usage\n",
        "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"  Final memory: {final_memory:.2f} MB\")\n",
        "    print(f\"  Reduction: {initial_memory - final_memory:.2f} MB ({(1 - final_memory/initial_memory)*100:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_mad(data):\n",
        "    \"\"\"\n",
        "    Calculate Median Absolute Deviation.\n",
        "    MAD = median(|X - median(X)|)\n",
        "    \"\"\"\n",
        "    median = np.nanmedian(data)\n",
        "    mad = np.nanmedian(np.abs(data - median))\n",
        "    return median, mad\n",
        "\n",
        "\n",
        "def remove_batch_outliers(df, mad_threshold=3.5):\n",
        "    \"\"\"\n",
        "    Remove batch-specific outliers using Median Absolute Deviation (MAD).\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    - mad_threshold: Number of MADs from median to consider outlier\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with outliers removed\n",
        "    \"\"\"\n",
        "    print(\"\\nRemoving batch-specific outliers using MAD...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "    outlier_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "    for batch in df['batch'].cat.categories:\n",
        "        batch_mask = df['batch'] == batch\n",
        "        batch_data = df.loc[batch_mask, 'expression_level'].dropna()\n",
        "\n",
        "        if len(batch_data) > 0:\n",
        "            median, mad = calculate_mad(batch_data.values)\n",
        "\n",
        "            # Calculate modified z-score\n",
        "            # Modified Z-score = 0.6745 * (X - median) / MAD\n",
        "            if mad > 0:\n",
        "                modified_z = 0.6745 * (df.loc[batch_mask, 'expression_level'] - median) / mad\n",
        "                batch_outliers = (np.abs(modified_z) > mad_threshold).fillna(False)\n",
        "                outlier_mask.loc[batch_mask] = batch_outliers.values\n",
        "\n",
        "    # Remove outliers\n",
        "    df_clean = df[~outlier_mask].copy()\n",
        "    outliers_removed = initial_count - len(df_clean)\n",
        "\n",
        "    print(f\"  Outliers removed: {outliers_removed:,} ({outliers_removed/initial_count*100:.2f}%)\")\n",
        "    print(f\"  Remaining rows: {len(df_clean):,}\")\n",
        "\n",
        "    return df_clean.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def impute_missing_values(df):\n",
        "    \"\"\"\n",
        "    Multi-step imputation strategy:\n",
        "    Step 1: For qc_flag == \"BAD\", impute with batch median\n",
        "    Step 2: For others, use KNN-style approach (vectorized for speed)\n",
        "    \"\"\"\n",
        "    print(\"\\nImputing missing values...\")\n",
        "\n",
        "    # Count missing values\n",
        "    missing_count = df['expression_level'].isna().sum()\n",
        "    print(f\"  Missing values: {missing_count:,} ({missing_count/len(df)*100:.2f}%)\")\n",
        "\n",
        "    # Step 1: Impute BAD qc_flag rows with batch median\n",
        "    print(\"\\n  Step 1: Imputing BAD qc_flag rows with batch median...\")\n",
        "\n",
        "    bad_missing_mask = (df['qc_flag'] == 'BAD') & (df['expression_level'].isna())\n",
        "    bad_missing_count = bad_missing_mask.sum()\n",
        "    print(f\"    BAD qc_flag missing: {bad_missing_count:,}\")\n",
        "\n",
        "    # Calculate batch medians\n",
        "    batch_medians = df.groupby('batch')['expression_level'].median()\n",
        "\n",
        "    # Impute BAD rows\n",
        "    for batch in df['batch'].cat.categories:\n",
        "        mask = bad_missing_mask & (df['batch'] == batch)\n",
        "        if mask.any():\n",
        "            df.loc[mask, 'expression_level'] = batch_medians[batch]\n",
        "\n",
        "    print(f\"    Imputed {bad_missing_count:,} values\")\n",
        "\n",
        "    # Step 2: KNN-style imputation for remaining missing values (vectorized)\n",
        "    print(\"\\n  Step 2: KNN-style imputation for remaining missing values...\")\n",
        "\n",
        "    remaining_missing = df['expression_level'].isna().sum()\n",
        "    print(f\"    Remaining missing: {remaining_missing:,}\")\n",
        "\n",
        "    if remaining_missing > 0:\n",
        "        # Calculate mean expression by batch and technician combination\n",
        "        print(\"    Calculating batch-technician means...\")\n",
        "        batch_tech_means = df.groupby(['batch', 'lab_technician'])['expression_level'].transform('mean')\n",
        "\n",
        "        # Fill missing values with batch-technician mean\n",
        "        missing_mask = df['expression_level'].isna()\n",
        "        df.loc[missing_mask, 'expression_level'] = batch_tech_means[missing_mask]\n",
        "\n",
        "        # For any still missing (rare case where batch-tech combo has no data), use batch mean\n",
        "        still_missing = df['expression_level'].isna()\n",
        "        if still_missing.any():\n",
        "            print(f\"    Using batch mean for {still_missing.sum():,} remaining values...\")\n",
        "            batch_means = df.groupby('batch')['expression_level'].transform('mean')\n",
        "            df.loc[still_missing, 'expression_level'] = batch_means[still_missing]\n",
        "\n",
        "        # Final fallback: global mean (very rare)\n",
        "        final_missing = df['expression_level'].isna()\n",
        "        if final_missing.any():\n",
        "            print(f\"    Using global mean for {final_missing.sum():,} final values...\")\n",
        "            df.loc[final_missing, 'expression_level'] = df['expression_level'].mean()\n",
        "\n",
        "    final_missing = df['expression_level'].isna().sum()\n",
        "    print(f\"\\n  Final missing values: {final_missing:,}\")\n",
        "    print(f\"  Total imputed: {missing_count - final_missing:,}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_gene_stability_index(df):\n",
        "    \"\"\"\n",
        "    Compute Gene Stability Index (GSI):\n",
        "    GSI = Std(expression_level) / Mean(expression_level)\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with gene_id and GSI, sorted by GSI (descending)\n",
        "    \"\"\"\n",
        "    print(\"\\nComputing Gene Stability Index...\")\n",
        "\n",
        "    # Group by gene and calculate statistics\n",
        "    gene_stats = df.groupby('gene_id')['expression_level'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "    # Calculate GSI\n",
        "    gene_stats['GSI'] = gene_stats['std'] / gene_stats['mean']\n",
        "\n",
        "    # Sort by GSI (descending - higher GSI = more unstable)\n",
        "    gene_stats = gene_stats.sort_values('GSI', ascending=False)\n",
        "\n",
        "    print(f\"  Computed GSI for {len(gene_stats):,} genes\")\n",
        "\n",
        "    return gene_stats\n",
        "\n",
        "\n",
        "def identify_top_unstable_genes(gene_stats, top_n=100):\n",
        "    \"\"\"\n",
        "    Identify top N most unstable genes.\n",
        "    \"\"\"\n",
        "    print(f\"\\nIdentifying top {top_n} unstable genes...\")\n",
        "\n",
        "    top_unstable = gene_stats.head(top_n).copy()\n",
        "\n",
        "    print(f\"\\nTop 10 most unstable genes:\")\n",
        "    print(top_unstable.head(10).to_string(index=False))\n",
        "\n",
        "    return top_unstable\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Genomics Analysis: Large-Scale Optimization + Missing-Value Strategies\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # Note: For demonstration, using 500K rows instead of 5M to avoid excessive runtime\n",
        "    # Change num_rows to 5_000_000 for full-scale analysis\n",
        "    NUM_ROWS = 500_000  # Use 500K for demo, change to 5M for production\n",
        "\n",
        "    print(f\"NOTE: Using {NUM_ROWS:,} rows for demonstration\")\n",
        "    print(\"      (Change NUM_ROWS to 5_000_000 for full 5M row analysis)\")\n",
        "    print()\n",
        "\n",
        "    # Step 1: Generate data\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Step 1: Generating Genomics Dataset\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    df = generate_genomics_data(num_rows=NUM_ROWS)\n",
        "\n",
        "    # Save raw data\n",
        "    print(\"\\nSaving raw data...\")\n",
        "    df.to_csv('genomics_raw.csv', index=False)\n",
        "    print(\"[OK] Raw data saved to: genomics_raw.csv\")\n",
        "\n",
        "    # Step 2: Optimize memory\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Step 2: Memory Optimization\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    df = optimize_memory(df)\n",
        "\n",
        "    # Step 3: Remove outliers\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Step 3: Removing Batch-Specific Outliers\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    df = remove_batch_outliers(df, mad_threshold=3.5)\n",
        "\n",
        "    # Step 4: Impute missing values\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Step 4: Multi-Step Missing Value Imputation\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    df = impute_missing_values(df)\n",
        "\n",
        "    # Step 5: Compute Gene Stability Index\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Step 5: Computing Gene Stability Index\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    gene_stats = compute_gene_stability_index(df)\n",
        "\n",
        "    # Step 6: Identify top unstable genes\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Step 6: Identifying Top Unstable Genes\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    top_unstable = identify_top_unstable_genes(gene_stats, top_n=100)\n",
        "\n",
        "    # Step 7: Export results\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Exporting Results\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # Save cleaned data\n",
        "    df.to_csv('genomics_cleaned.csv', index=False)\n",
        "    print(\"[OK] Cleaned data exported to: genomics_cleaned.csv\")\n",
        "\n",
        "    # Save gene statistics\n",
        "    gene_stats.to_csv('gene_stability_index.csv', index=False)\n",
        "    print(\"[OK] Gene stability index exported to: gene_stability_index.csv\")\n",
        "\n",
        "    # Save top unstable genes\n",
        "    top_unstable.to_csv('top_100_unstable_genes.csv', index=False)\n",
        "    print(\"[OK] Top 100 unstable genes exported to: top_100_unstable_genes.csv\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Summary Statistics\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"  Total rows: {len(df):,}\")\n",
        "    print(f\"  Unique genes: {df['gene_id'].nunique():,}\")\n",
        "    print(f\"  Unique samples: {df['sample_id'].nunique():,}\")\n",
        "    print(f\"  Batches: {df['batch'].nunique()}\")\n",
        "    print(f\"  Technicians: {df['lab_technician'].nunique()}\")\n",
        "\n",
        "    print(f\"\\nExpression Level Statistics:\")\n",
        "    print(f\"  Mean: {df['expression_level'].mean():.2f}\")\n",
        "    print(f\"  Std: {df['expression_level'].std():.2f}\")\n",
        "    print(f\"  Min: {df['expression_level'].min():.2f}\")\n",
        "    print(f\"  Max: {df['expression_level'].max():.2f}\")\n",
        "\n",
        "    print(f\"\\nTop 5 Most Unstable Genes:\")\n",
        "    for idx, row in top_unstable.head(5).iterrows():\n",
        "        print(f\"  {row['gene_id']}: GSI = {row['GSI']:.4f}\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Analysis Complete!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return df, gene_stats, top_unstable\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df, gene_stats, top_unstable = main()\n"
      ],
      "metadata": {
        "id": "ZrkPxIr8u6F0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}